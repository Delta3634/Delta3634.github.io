

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="John Doe">
  <meta name="keywords" content="">
  
    <meta name="description" content="（一）机器学习要素   训练集上表现良好，新样本集上表现较差：泛化能力差  一、分类  1.监督学习： 多个含有输入x和标签y的样本对组成，机器通过样本来学习正确答案     训练时，输入$x$送入映射函数$f(x)$，得到输出$y尖$，用正确答案**标签$y$**来监督$y尖$的偏差，从而修正函数，得到更准确的输出$y尖$;    监督学习的目标：构建从$输入x到输出y$之间的映射关系 解决的两">
<meta property="og:type" content="article">
<meta property="og:title" content="神经网络">
<meta property="og:url" content="http://example.com/2025/04/24/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/index.html">
<meta property="og:site_name" content="Delta">
<meta property="og:description" content="（一）机器学习要素   训练集上表现良好，新样本集上表现较差：泛化能力差  一、分类  1.监督学习： 多个含有输入x和标签y的样本对组成，机器通过样本来学习正确答案     训练时，输入$x$送入映射函数$f(x)$，得到输出$y尖$，用正确答案**标签$y$**来监督$y尖$的偏差，从而修正函数，得到更准确的输出$y尖$;    监督学习的目标：构建从$输入x到输出y$之间的映射关系 解决的两">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/2025/04/24/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%9B%BE%E7%89%871_%E5%9B%9B%E5%A4%A7%E8%A6%81%E7%B4%A0.png">
<meta property="og:image" content="http://example.com/2025/04/24/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%9B%BE%E7%89%872_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%88%86%E7%B1%BB.png">
<meta property="og:image" content="http://example.com/2025/04/24/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%9B%BE%E7%89%873_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%9A%84%E8%BF%87%E7%A8%8B.png">
<meta property="og:image" content="http://example.com/2025/04/24/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%9B%BE%E7%89%874.png">
<meta property="og:image" content="http://example.com/2025/04/24/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%9B%BE%E7%89%875_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0.png">
<meta property="og:image" content="http://example.com/2025/04/24/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%9B%BE%E7%89%876_%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0.png">
<meta property="og:image" content="http://example.com/2025/04/24/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%9B%BE%E7%89%877_%E4%B8%80%E4%B8%AA%E6%9E%84%E5%BB%BA%E5%87%BA%E6%9D%A5%E7%9A%84%E6%A8%A1%E5%9E%8B%E4%BE%8B%E5%AD%90.png">
<meta property="og:image" content="http://example.com/2025/04/24/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%9B%BE%E7%89%878.png">
<meta property="og:image" content="http://example.com/2025/04/24/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%9B%BE%E7%89%879_%E8%B6%85%E5%8F%82%E6%95%B0%E6%B6%89%E5%8F%8A%E7%9A%84%E9%97%AE%E9%A2%98.png">
<meta property="og:image" content="http://example.com/2025/04/24/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%9B%BE%E7%89%8710_%E4%B8%8D%E6%96%AD%E8%B0%83%E6%95%B4%E7%9B%B4%E5%88%B0%E6%89%BE%E5%88%B0%E8%B6%85%E5%8F%82%E6%95%B0%E7%9A%84%E8%BF%87%E7%A8%8B.png">
<meta property="og:image" content="http://example.com/2025/04/24/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%9B%BE%E7%89%8711_%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81.png">
<meta property="og:image" content="http://example.com/2025/04/24/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%9B%BE%E7%89%8712_%E5%B8%B8%E8%A7%84%E6%83%85%E5%86%B5%E4%B8%8B%E7%9A%84%E6%95%B0%E6%8D%AE%E9%9B%86%E5%88%92%E5%88%86%E6%83%85%E5%86%B5.png">
<meta property="og:image" content="http://example.com/2025/04/24/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%9B%BE%E7%89%8713_%E4%BD%BF%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%BE%97%E5%88%B0%E7%9A%84%E5%80%BC%E6%9C%80%E5%B0%8F%E5%8C%96.png">
<meta property="og:image" content="http://example.com/2025/04/24/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%9B%BE%E7%89%8714_%E5%87%B8%E5%87%BD%E6%95%B0.png">
<meta property="og:image" content="http://example.com/2025/04/24/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%9B%BE%E7%89%8715_%E9%9D%9E%E5%87%B8%E5%87%BD%E6%95%B0.png">
<meta property="og:image" content="http://example.com/2025/04/24/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%9B%BE%E7%89%8717_Sigmoid%E5%87%BD%E6%95%B0%E6%9B%B2%E7%BA%BF.png">
<meta property="og:image" content="http://example.com/2025/04/24/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%9B%BE%E7%89%8718_%E6%B5%85%E5%B1%82%E6%A8%A1%E5%9E%8B.png">
<meta property="og:image" content="http://example.com/2025/04/24/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%9B%BE%E7%89%8719_%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD.png">
<meta property="og:image" content="http://example.com/2025/04/24/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%9B%BE%E7%89%8720_%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD.png">
<meta property="og:image" content="http://example.com/2025/04/24/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%9B%BE%E7%89%8721_%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E5%88%B0w11%E7%9A%84%E6%83%85%E5%86%B5.png">
<meta property="og:image" content="http://example.com/2025/04/24/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%9B%BE%E7%89%8722_%E6%B1%82%E5%81%8F%E5%AF%BC%E7%9A%84%E8%BF%87%E7%A8%8B.png">
<meta property="og:image" content="http://example.com/2025/04/24/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%9B%BE%E7%89%8723_Sigmoid%E5%87%BD%E6%95%B0%E5%8F%8A%E5%85%B6%E5%AF%BC%E6%95%B0.png">
<meta property="og:image" content="http://example.com/2025/04/24/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%9B%BE%E7%89%8724_tahn%E5%87%BD%E6%95%B0%E5%8F%8A%E5%85%B6%E5%AF%BC%E6%95%B0.png">
<meta property="og:image" content="http://example.com/2025/04/24/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%9B%BE%E7%89%8725_Relu%E5%87%BD%E6%95%B0%E5%8F%8A%E5%85%B6%E5%AF%BC%E6%95%B0.png">
<meta property="og:image" content="http://example.com/2025/04/24/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%9B%BE%E7%89%8726_%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%97%B6%E5%8F%97%E5%BD%B1%E5%93%8D%E7%9A%84%E5%8E%9F%E5%9B%A0.png">
<meta property="og:image" content="http://example.com/2025/04/24/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%9B%BE%E7%89%8727_Leaky_ReLU%E7%9A%84%E8%A1%A8%E8%BE%BE%E5%BC%8F.png">
<meta property="og:image" content="http://example.com/2025/04/24/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%9B%BE%E7%89%8728.png">
<meta property="og:image" content="http://example.com/2025/04/24/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%9B%BE%E7%89%8729_ELU%E5%87%BD%E6%95%B0.png">
<meta property="og:image" content="http://example.com/2025/04/24/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%9B%BE%E7%89%8730_Swish%E5%87%BD%E6%95%B0.png">
<meta property="og:image" content="http://example.com/2025/04/24/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%9B%BE%E7%89%8731_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.png">
<meta property="og:image" content="http://example.com/2025/04/24/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%9B%BE%E7%89%8732.png">
<meta property="og:image" content="http://example.com/2025/04/24/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%9B%BE%E7%89%8733.png">
<meta property="og:image" content="http://example.com/2025/04/24/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%9B%BE%E7%89%8734.png">
<meta property="og:image" content="http://example.com/2025/04/24/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%9B%BE%E7%89%8735.png">
<meta property="og:image" content="http://example.com/2025/04/24/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%9B%BE%E7%89%8736_%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84%E5%8D%B7%E7%A7%AF%E8%BF%87%E7%A8%8B.png">
<meta property="og:image" content="http://example.com/2025/04/24/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%9B%BE%E7%89%8737.png">
<meta property="og:image" content="http://example.com/2025/04/24/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%9B%BE%E7%89%8738_%E4%B8%89%E9%80%9A%E9%81%93%E5%8D%B7%E7%A7%AF%E7%9A%84%E4%B8%80%E4%B8%AA%E4%BE%8B%E5%AD%90.png">
<meta property="og:image" content="http://example.com/2025/04/24/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%9B%BE%E7%89%8739_%E5%8D%B7%E7%A7%AF%E5%B1%82.png">
<meta property="og:image" content="http://example.com/2025/04/24/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%9B%BE%E7%89%8740.png">
<meta property="article:published_time" content="2025-04-24T10:09:23.000Z">
<meta property="article:modified_time" content="2025-04-24T15:21:34.033Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="学习笔记">
<meta property="article:tag" content="Markdown">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://example.com/2025/04/24/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%9B%BE%E7%89%871_%E5%9B%9B%E5%A4%A7%E8%A6%81%E7%B4%A0.png">
  
  
  
  <title>神经网络 - Delta</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.9.8","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":"Ege3qtslvoDkxjo0cgWTIfk4-gzGzoHsz","app_key":"XQm2hjsW66138F3PNUPluimQ","server_url":"https://Ege3qtslvoDkxjo0cgWTIfk4-gzGzoHsz.api.lncld.cn","path":"window.location.pathname","ignore_local":false},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  

  

  

  

  

  
    
  



  
<meta name="generator" content="Hexo 7.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Delta</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="神经网络"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-04-24 18:09" pubdate>
          2025年4月24日 晚上
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          5.4k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          45 分钟
        
      </span>
    

    
    
      
        <span id="leancloud-page-views-container" class="post-meta" style="display: none">
          <i class="iconfont icon-eye" aria-hidden="true"></i>
          <span id="leancloud-page-views"></span> 次
        </span>
        
      
      
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">神经网络</h1>
            
            
              <div class="markdown-body">
                
                <h1 id="（一）机器学习要素"><a href="#（一）机器学习要素" class="headerlink" title="（一）机器学习要素"></a>（一）机器学习要素</h1><img src="/2025/04/24/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%9B%BE%E7%89%871_%E5%9B%9B%E5%A4%A7%E8%A6%81%E7%B4%A0.png" srcset="/img/loading.gif" lazyload class="">

<ul>
<li>训练集上表现良好，新样本集上表现较差：<strong>泛化能力差</strong></li>
</ul>
<h2 id="一、分类"><a href="#一、分类" class="headerlink" title="一、分类"></a>一、分类</h2><img src="/2025/04/24/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%9B%BE%E7%89%872_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%88%86%E7%B1%BB.png" srcset="/img/loading.gif" lazyload class="">

<h3 id="1-监督学习："><a href="#1-监督学习：" class="headerlink" title="1.监督学习："></a>1.监督学习：</h3><ul>
<li><strong>多个含有输入x和标签y的样本对组成</strong>，机器通过样本来学习正确答案</li>
</ul>
<img src="/2025/04/24/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%9B%BE%E7%89%873_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%9A%84%E8%BF%87%E7%A8%8B.png" srcset="/img/loading.gif" lazyload class="">

<ul>
<li>训练时，输入$x$送入映射函数$f(x)$，得到输出$y尖$，用正确答案**标签$y$**来监督$y尖$的偏差，从而修正函数，得到更准确的输出$y尖$;</li>
</ul>
<img src="/2025/04/24/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%9B%BE%E7%89%874.png" srcset="/img/loading.gif" lazyload class="">

<h4 id="监督学习的目标："><a href="#监督学习的目标：" class="headerlink" title="监督学习的目标："></a>监督学习的目标：</h4><p>构建从$输入x到输出y$之间的映射关系</p>
<h4 id="解决的两种问题：分类-回归"><a href="#解决的两种问题：分类-回归" class="headerlink" title="解决的两种问题：分类&amp;回归"></a>解决的两种问题：分类&amp;回归</h4><ul>
<li><strong>分类</strong>：对样本进行区分（根据标签——机器只能识别数字，所以将标签对应到具体的数字上），输出内容为<strong>离散值</strong></li>
<li><strong>回归</strong>：预测一个<strong>连续的值</strong>，例如：股价预测——<strong>用特征向量</strong>（包含了n个特征，每个特征都是一维的，n维特征向量，所有的特征都要转为数值来处理）<strong>来表征待预测的对象</strong></li>
</ul>
<h3 id="2-无监督学习："><a href="#2-无监督学习：" class="headerlink" title="2.无监督学习："></a>2.无监督学习：</h3><ul>
<li><p>训练集只有$输入x$而没有$标签y$——没有正确答案来监督输出</p>
<img src="/2025/04/24/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%9B%BE%E7%89%875_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0.png" srcset="/img/loading.gif" lazyload class=""></li>
</ul>
<h4 id="无监督学习常见的任务"><a href="#无监督学习常见的任务" class="headerlink" title="无监督学习常见的任务"></a>无监督学习常见的任务</h4><p><strong>聚类</strong>，自行挖掘出数据的内在结构，讲给定的数据分成不同的类别</p>
<h3 id="3-半监督学习"><a href="#3-半监督学习" class="headerlink" title="3.半监督学习"></a>3.半监督学习</h3><img src="/2025/04/24/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%9B%BE%E7%89%876_%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0.png" srcset="/img/loading.gif" lazyload class="">

<p>使用场景：<strong>标签数据比较昂贵，或有稳定数据流</strong>；</p>
<p>例如：检测社交网络的违规信息（手动标注部分数据，用半监督技术来修正对位标注数据的理解）；</p>
<h3 id="4-强化学习"><a href="#4-强化学习" class="headerlink" title="4.强化学习"></a>4.强化学习</h3><p>机器通过与环境的交互来实现目标的一种算法</p>
<h2 id="二、数据集的划分"><a href="#二、数据集的划分" class="headerlink" title="二、数据集的划分"></a>二、数据集的划分</h2><h3 id="1-训练集"><a href="#1-训练集" class="headerlink" title="1.训练集"></a>1.训练集</h3><p>上课学的东西，用来<strong>训练模型</strong>，学会知识点</p>
<h3 id="2-验证集"><a href="#2-验证集" class="headerlink" title="2.验证集"></a>2.验证集</h3><p>课后习题，用来<strong>纠正和强化</strong>知识点，用来评估不同超参数训练出的模型效果，从而优化模型</p>
<h3 id="3-测试集"><a href="#3-测试集" class="headerlink" title="3.测试集"></a>3.测试集</h3><p>（闭卷！）考试，用来<strong>评估最终模型的效果</strong></p>
<h2 id="三、模型"><a href="#三、模型" class="headerlink" title="三、模型"></a>三、模型</h2><ul>
<li>模型是机器学习的结果</li>
<li>它描述了从输入到输出之间的映射关系</li>
<li>可以理解为是一个函数</li>
<li><strong>本质</strong>：算法＋参数的组合</li>
</ul>
<img src="/2025/04/24/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%9B%BE%E7%89%877_%E4%B8%80%E4%B8%AA%E6%9E%84%E5%BB%BA%E5%87%BA%E6%9D%A5%E7%9A%84%E6%A8%A1%E5%9E%8B%E4%BE%8B%E5%AD%90.png" srcset="/img/loading.gif" lazyload class="">

<img src="/2025/04/24/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%9B%BE%E7%89%878.png" srcset="/img/loading.gif" lazyload class="">

<ul>
<li><strong>超参数</strong>：训练模型前，还需要设定其他的参数</li>
</ul>
<img src="/2025/04/24/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%9B%BE%E7%89%879_%E8%B6%85%E5%8F%82%E6%95%B0%E6%B6%89%E5%8F%8A%E7%9A%84%E9%97%AE%E9%A2%98.png" srcset="/img/loading.gif" lazyload class="">

<p>————很难用解析解&#x2F;梯度下降等传统方式获得，一般只能不断试错得到（但上面的a,b也属于超参数）</p>
<p><strong>测试集和训练集要严格分开，所以先将测试集固定下来</strong>，之后再讲剩下的数据划分为训练集和验证集，<strong>训练集用来训练模型，验证集用来评估模型，不断地调整超参数以找到最终的&#x3D;&#x3D;最优超参数&#x3D;&#x3D;</strong></p>
<img src="/2025/04/24/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%9B%BE%E7%89%8710_%E4%B8%8D%E6%96%AD%E8%B0%83%E6%95%B4%E7%9B%B4%E5%88%B0%E6%89%BE%E5%88%B0%E8%B6%85%E5%8F%82%E6%95%B0%E7%9A%84%E8%BF%87%E7%A8%8B.png" srcset="/img/loading.gif" lazyload class="">

<p><strong>数据集足够时</strong>，我们采取上述调参流程，不断调整，最终使得<strong>模型评估值最高</strong>的超参数值即为我们要找的<strong>最优超参数</strong>；</p>
<p><strong>数据值不足一万时</strong>，采用<strong>交叉验证</strong>（——》<strong>训练集与验证集</strong>之间的数据相互转换《——）</p>
<ul>
<li><strong>K折交叉验证</strong>——将训练集分为K份的交叉验证方式</li>
</ul>
<img src="/2025/04/24/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%9B%BE%E7%89%8711_%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81.png" srcset="/img/loading.gif" lazyload class="">

<p>此时在<strong>均值曲线</strong>上，使得模型评估值最大的超参数值即为最优超参数值</p>
<p>出现多个超参数时，采用的就是一个超参数组合</p>
<ul>
<li><p><strong>留一交叉验证</strong>——（样本量极少时——<em><strong>小于50</strong></em>才用）缺乏样本的情况下，每次只取一个样本作为验证集，此时的K和数据个数相等</p>
</li>
<li><p><strong>常规的数据集划分比例</strong>：</p>
</li>
</ul>
<img src="/2025/04/24/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%9B%BE%E7%89%8712_%E5%B8%B8%E8%A7%84%E6%83%85%E5%86%B5%E4%B8%8B%E7%9A%84%E6%95%B0%E6%8D%AE%E9%9B%86%E5%88%92%E5%88%86%E6%83%85%E5%86%B5.png" srcset="/img/loading.gif" lazyload class="">



<ul>
<li>数据集数量庞大时不可这样划分：<ul>
<li>验证集是为了得到最优超参数，1万左右的数据即可达到效果；</li>
<li>测试集是为了验证模型正确性，1万左右的数据即具有代表性；</li>
<li>划分比例变为——$100:1:1$</li>
</ul>
</li>
</ul>
<h2 id="四、损失函数——得到的值无正负之分"><a href="#四、损失函数——得到的值无正负之分" class="headerlink" title="四、损失函数——得到的值无正负之分"></a>四、损失函数——得到的值无正负之分</h2><ul>
<li><strong>量化模型预测值与标签真实值之间的差异</strong>，帮助筛选出更好的模型</li>
</ul>
<h2 id="五、最终转化成最优化问题"><a href="#五、最终转化成最优化问题" class="headerlink" title="五、最终转化成最优化问题"></a>五、最终转化成最优化问题</h2><img src="/2025/04/24/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%9B%BE%E7%89%8713_%E4%BD%BF%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%BE%97%E5%88%B0%E7%9A%84%E5%80%BC%E6%9C%80%E5%B0%8F%E5%8C%96.png" srcset="/img/loading.gif" lazyload class="">

<ul>
<li><p>目标：找到一组$\theta$，使$样本x到映射值y尖$之间的偏差最小</p>
</li>
<li><p>最优化问题中有两种函数：</p>
<ul>
<li><p><strong>凸函数</strong>：任意两点连线组成的线段都在这两点的函数曲线（曲面）的同一方向——<strong>凸函数只有一个全局最优解</strong></p>
<img src="/2025/04/24/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%9B%BE%E7%89%8714_%E5%87%B8%E5%87%BD%E6%95%B0.png" srcset="/img/loading.gif" lazyload class="">

<p><strong>求导找到最值点则可找到全局最优解</strong></p>
<p><strong>沿着梯度下降的方向更新参数，寻找最优解</strong></p>
</li>
<li><p><strong>非凸函数</strong>：</p>
<img src="/2025/04/24/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%9B%BE%E7%89%8715_%E9%9D%9E%E5%87%B8%E5%87%BD%E6%95%B0.png" srcset="/img/loading.gif" lazyload class=""></li>
</ul>
</li>
</ul>
<h1 id="（二）神经网络模型"><a href="#（二）神经网络模型" class="headerlink" title="（二）神经网络模型"></a>（二）神经网络模型</h1><ul>
<li><p>一种机器学习（有监督学习）模型；</p>
</li>
<li><p><strong>深度学习</strong>：深度神经网络（由深的层构成的神经网络模型）</p>
</li>
<li><p>神经网络模型由<strong>神经元模型</strong>为单位组成</p>
</li>
</ul>
<h2 id="一、神经元模型"><a href="#一、神经元模型" class="headerlink" title="一、神经元模型"></a>一、神经元模型</h2><ol>
<li><strong>变量</strong>：<ul>
<li>输入维度$D$</li>
<li>$D&#x3D;2$时，两个输入$x_0$和$x_1$</li>
<li>各自的权重$w_0$和$w_1$</li>
<li>还有一个值永远为1的虚拟输入（<strong>偏置函数</strong>），其对应的权重$w_2$为偏置参数、偏置项</li>
<li><strong>激活函数$f$</strong>：早期使用<strong>Sigmoid函数</strong>——一种常用的S型（Sigmoidal）激活函数<ul>
<li>作用：将任意实数输入映射到**$(0,1)$**的区间内，输出值可以解释为概率</li>
<li>数学定义：$\sigma(x)&#x3D;\frac{1}{1+e^{-x}}$</li>
<li>$x$为总输入值，即个实际输入量经过加权求和之后得到的值</li>
<li>特性：输出范围（0,1），适合表示概率；严格递增函数；处处可导（<strong>在反向传播中便于计算梯度</strong>）</li>
</ul>
</li>
</ul>
</li>
</ol>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-comment"># Sigmoid函数</span><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">sigmoid</span>(<span class="hljs-params">x</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">1</span>/(<span class="hljs-number">1</span>+np.exp(-x))<br><br>x = np.linspace(-<span class="hljs-number">5</span>,<span class="hljs-number">5</span>,<span class="hljs-number">100</span>)<br>y = sigmoid(x)<br>plt.plot(x,y)<br>plt.title(<span class="hljs-string">&quot;Sigmoid Function&quot;</span>)<br>plt.xlabel(<span class="hljs-string">&quot;x&quot;</span>)<br>plt.ylabel(<span class="hljs-string">&quot;σ(x)&quot;</span>)<br>plt.grid()<br>plt.show()<br></code></pre></td></tr></table></figure>

<img src="/2025/04/24/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%9B%BE%E7%89%8717_Sigmoid%E5%87%BD%E6%95%B0%E6%9B%B2%E7%BA%BF.png" srcset="/img/loading.gif" lazyload class="">

<ul>
<li>在0的右侧，x越大，输出越接近1；在0的左侧，x越大，输出越接近0；</li>
</ul>
<ol start="2">
<li><strong>一个神经元的功能是求得输入向量与权向量的内积后，经一个非线性传递函数得到一个标量结果</strong></li>
</ol>
<h2 id="二、神经网络模型"><a href="#二、神经网络模型" class="headerlink" title="二、神经网络模型"></a>二、神经网络模型</h2><ul>
<li><strong>浅层神经网络</strong>：只包含输入层、隐藏层和输出层</li>
</ul>
<img src="/2025/04/24/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%9B%BE%E7%89%8718_%E6%B5%85%E5%B1%82%E6%A8%A1%E5%9E%8B.png" srcset="/img/loading.gif" lazyload class="">

<h3 id="1-分类"><a href="#1-分类" class="headerlink" title="1.分类"></a>1.分类</h3><ul>
<li><p><strong>前馈神经网络</strong>——信号从输入层向输出层单向传输的网络</p>
<ul>
<li><p>信号传输：<strong>前向传播</strong>、<strong>反向传播（BP算法）</strong></p>
<ul>
<li><p>&#x3D;&#x3D;前向传播&#x3D;&#x3D;：信号传入模型，经过一层层运算得到输出</p>
</li>
<li><img src="/2025/04/24/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%9B%BE%E7%89%8719_%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD.png" srcset="/img/loading.gif" lazyload class="">

<p>输入层、输出层都有两个神经元：说明输入和输出的数据都是二维的</p>
<p>在这里我们在隐藏层和输出层都采用$Sigmoid函数$来作为激活函数</p>
</li>
<li><p>&#x3D;&#x3D;<strong>反向传播</strong>&#x3D;&#x3D;：输出层往回传（<strong>前向传播的相反过程，所以回传的时候要考虑清楚</strong>）——<strong>用于传输误差</strong></p>
<img src="/2025/04/24/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%9B%BE%E7%89%8720_%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD.png" srcset="/img/loading.gif" lazyload class="">

<p>往回传，$E_{o1}$先经过$o_1$的激活单元再经过加权单元，最后再传给$w20$——过程本质上是<strong>链式法则求偏导</strong></p>
<img src="/2025/04/24/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%9B%BE%E7%89%8721_%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E5%88%B0w11%E7%9A%84%E6%83%85%E5%86%B5.png" srcset="/img/loading.gif" lazyload class="">

<img src="/2025/04/24/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%9B%BE%E7%89%8722_%E6%B1%82%E5%81%8F%E5%AF%BC%E7%9A%84%E8%BF%87%E7%A8%8B.png" srcset="/img/loading.gif" lazyload class="">

<p>红色公式：梯度下降法的权重更新公式更新$w_11$的值</p>
</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>循环神经网络（记忆网络）</strong>——信号除了向后传输之外还会回传给自身的网络，当前神经元节点上$t时刻$的输出值在$t+1$时刻还会作为输入重新传输回来，使得网络<strong>具备记忆能力</strong></p>
</li>
<li><p><strong>图神经网络</strong>——由一个额外的图来定义，基本单位：&#x3D;&#x3D;一组神经元&#x3D;&#x3D;</p>
</li>
</ul>
<h3 id="2-网络结构"><a href="#2-网络结构" class="headerlink" title="2.网络结构"></a>2.网络结构</h3><ul>
<li>要素：网络层数（网络深度）、每层神经元的个数（网络宽度）</li>
</ul>
<p>——网络深度增加，模型能够拟合更复杂的函数，完成更复杂的任务，但运算量也暴增、梯度消失、梯度爆炸</p>
<p>————需要根据应用场景和综合算力来综合权衡</p>
<h3 id="3-激活函数"><a href="#3-激活函数" class="headerlink" title="3.激活函数"></a>3.激活函数</h3><p>前向激活、反向梯度回传（链式法则）</p>
<h4 id="（1）-Sigmoid-函数"><a href="#（1）-Sigmoid-函数" class="headerlink" title="（1）$Sigmoid$函数"></a>（1）$Sigmoid$函数</h4><img src="/2025/04/24/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%9B%BE%E7%89%8723_Sigmoid%E5%87%BD%E6%95%B0%E5%8F%8A%E5%85%B6%E5%AF%BC%E6%95%B0.png" srcset="/img/loading.gif" lazyload class="">

<ul>
<li><strong>双侧饱和</strong></li>
<li>全为正梯度饱和会影响神经网络的收敛，会出现<strong>梯度消失</strong></li>
</ul>
<h4 id="（2）-tanh-函数"><a href="#（2）-tanh-函数" class="headerlink" title="（2）$tanh$函数"></a>（2）$tanh$函数</h4><p>——又叫双曲正切函数：</p>
<p>$tanh(x)&#x3D;\frac{e^x-e^{-x}}{e^x+e^{-x}}$</p>
<img src="/2025/04/24/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%9B%BE%E7%89%8724_tahn%E5%87%BD%E6%95%B0%E5%8F%8A%E5%85%B6%E5%AF%BC%E6%95%B0.png" srcset="/img/loading.gif" lazyload class="">

<ul>
<li><p>有正有负，可以有效解决$sigmoid函数恒大于0$带来的收敛慢的问题</p>
</li>
<li><p>但也存在梯度饱和，所以也会出现梯度消失</p>
</li>
<li><p>运算效率</p>
</li>
</ul>
<h4 id="（3）-ReLU-函数"><a href="#（3）-ReLU-函数" class="headerlink" title="（3）$ReLU$函数"></a>（3）$ReLU$函数</h4><p>——又叫线性整流函数</p>
<p>$ReLU(x)&#x3D;max(0,x)$</p>
<p>解决了梯度消失和运算效率的问题</p>
<img src="/2025/04/24/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%9B%BE%E7%89%8725_Relu%E5%87%BD%E6%95%B0%E5%8F%8A%E5%85%B6%E5%AF%BC%E6%95%B0.png" srcset="/img/loading.gif" lazyload class="">

<ul>
<li><strong>单侧饱和</strong></li>
<li>函数输入为负值时，输出为0；</li>
</ul>
<p>​        函数输入为正值时，输出等于输入值；</p>
<ul>
<li>函数输入为正值时，梯度值恒为1——有效解决了梯度消失的问题；</li>
<li>只存在<strong>线性关系</strong>，计算量比$sigmoid和tanh函数$少很多；</li>
<li><strong>缺陷</strong>：学习率过大时，一些神经元会永久性失活——影响网络正常更新</li>
</ul>
<img src="/2025/04/24/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%9B%BE%E7%89%8726_%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%97%B6%E5%8F%97%E5%BD%B1%E5%93%8D%E7%9A%84%E5%8E%9F%E5%9B%A0.png" srcset="/img/loading.gif" lazyload class="">

<p>学习率过大时，会导致$\mu\Delta w^t$大于$w^t$的情况，使得$w^{t+1}$的值变为负值</p>
<p>——》权重为负值时，输入网络的正值会在和权重相乘后也变为负值</p>
<p>——》负值经过$ReLU$变为0，对应的$ReLU$导数也为0</p>
<p>——》导致$\mu\Delta w^t$为0</p>
<p>——》$w$得不到更新，一直为负</p>
<p>——》其神经元一直输出0，永久失活</p>
<p>——》<strong>死亡ReLU问题</strong></p>
<h4 id="（4）-Leaky-ReLU-函数"><a href="#（4）-Leaky-ReLU-函数" class="headerlink" title="（4）$Leaky\  ReLU$函数"></a>（4）$Leaky\  ReLU$函数</h4><img src="/2025/04/24/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%9B%BE%E7%89%8727_Leaky_ReLU%E7%9A%84%E8%A1%A8%E8%BE%BE%E5%BC%8F.png" srcset="/img/loading.gif" lazyload class="">

<p>针对<strong>Dead ReLU</strong>而提出的激活函数</p>
<img src="/2025/04/24/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%9B%BE%E7%89%8728.png" srcset="/img/loading.gif" lazyload class="">

<hr>
<p><strong>好的激活函数</strong>应该满足两个条件：</p>
<ul>
<li>单侧饱和</li>
<li>输出值分布在0的两侧</li>
</ul>
<hr>
<h4 id="（5）-ELU-函数"><a href="#（5）-ELU-函数" class="headerlink" title="（5）$ELU$函数"></a>（5）$ELU$函数</h4><img src="/2025/04/24/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%9B%BE%E7%89%8729_ELU%E5%87%BD%E6%95%B0.png" srcset="/img/loading.gif" lazyload class="">

<h4 id="（6）-swish-函数"><a href="#（6）-swish-函数" class="headerlink" title="（6）$swish$函数"></a>（6）$swish$函数</h4><p>表达式：$Swish(x)&#x3D;x\cdot \sigma(\beta \cdot x)$</p>
<ul>
<li>$\sigma$：$Sigmoid函数$</li>
<li>$\beta$：可调节参数</li>
<li>$\sigma(\beta \cdot x)$：$\frac{1}{1+e^{-\beta x}}$</li>
</ul>
<img src="/2025/04/24/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%9B%BE%E7%89%8730_Swish%E5%87%BD%E6%95%B0.png" srcset="/img/loading.gif" lazyload class="">

<ul>
<li><p>$Sigmoid$ 函数：起到了门控单元的作用</p>
</li>
<li><p>先通过门控单元得到的输出在于输入信号相乘得到输出值</p>
</li>
<li><p><strong>满足单侧饱和、输出值分布在0的两侧</strong></p>
</li>
</ul>
<h2 id="三、全连接神经网络——多层感知机"><a href="#三、全连接神经网络——多层感知机" class="headerlink" title="三、全连接神经网络——多层感知机"></a>三、全连接神经网络——多层感知机</h2><p>有缺陷</p>
<h2 id="四、卷积神经网络（CNN）"><a href="#四、卷积神经网络（CNN）" class="headerlink" title="四、卷积神经网络（CNN）"></a>四、卷积神经网络（CNN）</h2><img src="/2025/04/24/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%9B%BE%E7%89%8731_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.png" srcset="/img/loading.gif" lazyload class="">

<p>——<strong>特征提取器</strong>，能够对图像、音频等数据进行特征提取（卷积赋予的）</p>
<h3 id="1-系统分类："><a href="#1-系统分类：" class="headerlink" title="1.系统分类："></a>1.系统分类：</h3><ul>
<li><strong>无记忆系统</strong>：系统输出只与当前输入有关 $y_t&#x3D;f(x_t)$<ul>
<li>e.g 信号放大器</li>
</ul>
</li>
<li><strong>记忆系统</strong>：输出与当前输入及历史输入都有关系</li>
</ul>
<h3 id="2-卷积：用来计算有记忆系统的输出问题"><a href="#2-卷积：用来计算有记忆系统的输出问题" class="headerlink" title="2.卷积：用来计算有记忆系统的输出问题"></a>2.<strong>卷积</strong>：用来计算<strong>有记忆系统的输出问题</strong></h3><ul>
<li><p>系统输出要满足的两个条件：</p>
<ul>
<li>线性——输入和输出成比例</li>
<li>时不变——不同的时刻，相同的输入应得到相同的输出</li>
</ul>
</li>
<li><p>为什么要<strong>卷</strong>（<strong>对响应函数先进行翻转</strong>）：系统有处理时间延迟，所以信号衰减了，这个时刻的输出是前一段时刻输入的总信号的总和影响的</p>
</li>
<li><p><strong>离散卷积（一维）</strong>：$y(t)&#x3D;f(t)*g(t)&#x3D;\sum^{+\infty}_{i&#x3D;-\infty}f(i)\cdot g(t-i)$</p>
<ul>
<li><p>此处的$*$：表示卷积运算</p>
</li>
<li><p>$y(t)$：$t$时刻的响应输出</p>
</li>
<li><p>例子：</p>
<img src="/2025/04/24/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%9B%BE%E7%89%8732.png" srcset="/img/loading.gif" lazyload class="">

<p>$g(t)变为g(-t)$，计算t时刻的响应输出，再把$g(-t)$平移$t$个单位</p>
<img src="/2025/04/24/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%9B%BE%E7%89%8733.png" srcset="/img/loading.gif" lazyload class="">

<ul>
<li>卷积结果就等于0和1时刻两函数对应值分别相乘，乘积再相加的结果</li>
</ul>
<img src="/2025/04/24/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%9B%BE%E7%89%8734.png" srcset="/img/loading.gif" lazyload class="">

<p>假设在$T&#x3D;6$时刻运算结束，就可以得到一个长度为7的输出序列</p>
<img src="/2025/04/24/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%9B%BE%E7%89%8735.png" srcset="/img/loading.gif" lazyload class=""></li>
</ul>
</li>
<li><p><strong>连续卷积（一维）</strong>：$y(t)&#x3D;\int ^{+\infty}_{-\infty}f(i)\cdot g(t-i)di$</p>
<ul>
<li>$i$：积分变量</li>
<li>$t$：函数$g(-i)$平移的量</li>
</ul>
</li>
<li><p><strong>二维 离散卷积</strong>：$y(i,j)&#x3D;f(i,j)*g(i,j)&#x3D;\sum_m \sum_n f(m,n)\cdot g(i-m,j-n)$</p>
</li>
<li><p><strong>响应函数的处理</strong>：上下、左右翻转，之后先沿横坐标方向，再沿纵坐标方向平移</p>
</li>
</ul>
<h3 id="3-卷积运算过程"><a href="#3-卷积运算过程" class="headerlink" title="3.卷积运算过程"></a>3.卷积运算过程</h3><h4 id="（1）单通道的二维数据："><a href="#（1）单通道的二维数据：" class="headerlink" title="（1）单通道的二维数据："></a>（1）<strong>单通道的二维数据：</strong></h4><img src="/2025/04/24/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%9B%BE%E7%89%8736_%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84%E5%8D%B7%E7%A7%AF%E8%BF%87%E7%A8%8B.png" srcset="/img/loading.gif" lazyload class="">

<ul>
<li><strong>特征图</strong>：</li>
<li><strong>权重</strong>：卷积核</li>
<li><strong>步长</strong>：超参数，在设计卷积层时手动设定（——》影响卷积大小和尺寸——》影响特征提取的效果和运算量<ul>
<li>要压缩信息、减小输出尺寸时：设定较大的步长</li>
</ul>
</li>
<li><strong>填充（padding）</strong>：超参数——卷积操作会使得图像逐渐减小（特征图减小到1×1时就没办法再卷积）+<strong>图像边缘处的信息没有被充分利用</strong>（边缘的只参与一次卷积运算，影响一个输出结果）——》padding出现，来解决这些问题<ul>
<li><strong>padding的具体操作</strong>：沿着图像边缘做填充</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># padding值为1：表示填充一个像素</span><br><span class="hljs-comment"># ————》填充：上下左右（边）各填充一个像素</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">####</span><br><span class="hljs-string">#  #</span><br><span class="hljs-string">#  #</span><br><span class="hljs-string">####</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-comment"># padding职为2：表示填充两个像素</span><br><span class="hljs-comment"># ————》图像周围填充两个像素</span><br></code></pre></td></tr></table></figure>

<ul>
<li><strong>目的</strong>：填充（padding）后，得到的特征图大小不变</li>
</ul>
<img src="/2025/04/24/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%9B%BE%E7%89%8737.png" srcset="/img/loading.gif" lazyload class="">

<p><strong>&#x3D;&#x3D;$w_{out}&#x3D;floor(\frac{w_{in}+2*padding-F}{stride})+1$&#x3D;&#x3D;</strong>    ——》默认宽高相同，不同时则需要将宽和高分别带入式子计算得出输出尺寸</p>
<ul>
<li>$w_{out}$：输出尺寸</li>
<li>$w_{in}$：输入尺寸</li>
<li>$padding$：填充像素个数</li>
<li>$F$：卷积核大小</li>
<li>$stride$：步长</li>
<li>$floor$：向下取整</li>
</ul>
<hr>
<h4 id="（2）多通道卷积："><a href="#（2）多通道卷积：" class="headerlink" title="（2）多通道卷积："></a>（2）<strong>多通道卷积</strong>：</h4><p>卷积核的通道个数要和输入数据的通道个数一致，卷积结果为所有通道的卷积和，<strong>最终输出仍为一个二维的特征图</strong>；</p>
<p>e.g：RGB——<strong>三通道输入，但只有一个卷积核——所以输出只有一个通道</strong></p>
<img src="/2025/04/24/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%9B%BE%E7%89%8738_%E4%B8%89%E9%80%9A%E9%81%93%E5%8D%B7%E7%A7%AF%E7%9A%84%E4%B8%80%E4%B8%AA%E4%BE%8B%E5%AD%90.png" srcset="/img/loading.gif" lazyload class="">

<ul>
<li>各通道计算得到的值相加则为输出中对应的值</li>
</ul>
<p><em><strong>实际中，每一层都会有&#x3D;&#x3D;多个卷积核&#x3D;&#x3D;参与计算</strong></em></p>
<p>每个卷积核分别于输入卷积进行运算得到一个单通道的输出</p>
<p>——》多个单通道的输出拼在一起</p>
<p>——》多通道的输出</p>
<img src="/2025/04/24/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%9B%BE%E7%89%8739_%E5%8D%B7%E7%A7%AF%E5%B1%82.png" srcset="/img/loading.gif" lazyload class="">

<ul>
<li>卷积层的参数：卷积核的大小、数量及步长</li>
</ul>
<p>——》卷积层的权重参数可以通过训练来优化——》使卷积层能更好地提取输入数据中的特征</p>
<h3 id="4-卷积层"><a href="#4-卷积层" class="headerlink" title="4.卷积层"></a>4.卷积层</h3><ul>
<li><strong>边缘特征提取</strong>：图像分类、目标检测的基本任务<ul>
<li>边缘：灰度值、颜色或纹理结构等突变的地方——表示一个区域的结束，也表示另一个区域的开始</li>
</ul>
</li>
</ul>
<img src="/2025/04/24/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%9B%BE%E7%89%8740.png" srcset="/img/loading.gif" lazyload class="">

<p>一阶导变化较大，二阶导经过零点</p>
<ul>
<li><p>连续一阶微分：$lim_{\epsilon \rightarrow 0}\frac{f(x+\epsilon)-f(x)}{\epsilon}$ 或 $lim_{\epsilon \rightarrow 0}\frac{f(x+\epsilon)-f(x-\epsilon)}{2\epsilon}$</p>
</li>
<li><p>离散一阶微分：$y^{‘}(t)&#x3D;\frac{y(t+1)-y(t-1)}{2}$   </p>
</li>
<li><p>卷积核：$[- \frac{1}{2},0,\frac{1}{2}]$，系数$\frac{1}{2}$可以舍弃，变为$[-1,0,1]$，与输入层相乘便可得到边缘区域的边缘特征&#x3D;&#x3D;（？）&#x3D;&#x3D;</p>
</li>
<li><p>灰度图像：可看做一个<strong>二维离散函数</strong></p>
<ul>
<li>图像中任意一行：看做<strong>像素点随着$x轴$变化的信号</strong></li>
<li>图像中任意一列：看做<strong>像素点随着$y轴$变化的信号</strong></li>
</ul>
</li>
</ul>
<h4 id="卷积操作的两大特点："><a href="#卷积操作的两大特点：" class="headerlink" title="卷积操作的两大特点："></a>卷积操作的两大特点：</h4><ul>
<li><p><strong>局部感受野</strong>：感受野——神经元在初始输入数据上所覆盖的区域——即在计算时所看到的输入计算的部分</p>
<ul>
<li><p>卷积层中，每一层只和上一层的部分区域有连接——局部——较小的感受野</p>
</li>
<li><p>网络层逐渐加深，<strong>卷积+池化</strong>操作——》使得神经元的感受野逐渐扩大——》下一层对上一层的信息进行整合——》最深层对所有信息进行整合——》得到全局的信息——》用于最终的决策任务（分类、检测、分割等）——》此时，<strong>神经元的感受野可以覆盖整个图像</strong></p>
<p>——层次化学习机制，使得卷积神经网络既可以<strong>感知局部信息</strong>，又有<strong>获取全局信息</strong>的能力</p>
<p>——从通道上说：每个输出神经元和所有通道保持全连接——so，<strong>卷积层是特殊的全连接层</strong></p>
</li>
</ul>
</li>
<li><p><strong>权重共享机制</strong></p>
<ul>
<li>都用的一个卷积核（权重共享——减少了参数量，降低过拟合风险，提高泛化能力）</li>
<li>卷积核的通道数和输入通道数相同，卷积核的个数和输出通道相同</li>
</ul>
</li>
</ul>
<h3 id="5-池化层"><a href="#5-池化层" class="headerlink" title="5.池化层"></a>5.池化层</h3><ul>
<li>定义：把一个窗口中的所有特征信息浓缩为一个输出</li>
</ul>
<p>——》<strong>减小输出尺寸</strong></p>
<p>——》在模型结构中增加池化层，可以<strong>提高模型对输入图像中物体平移变化的鲁棒性</strong>——<strong>平移不变性</strong></p>
<ul>
<li><p>和卷积类似，都是通过<strong>滑动窗口来进行运算</strong>的，但<strong>不需要权重参数</strong>，池化操作的每个通道都是单独计算的</p>
</li>
<li><p>输入通道数&#x3D;输出通道数</p>
<h4 id="常见池化操作："><a href="#常见池化操作：" class="headerlink" title="常见池化操作："></a><strong>常见池化操作</strong>：</h4><ul>
<li><strong>平均池化（Mean Pooling）</strong>：在窗口内计算平均值作为输出</li>
<li><strong>最大池化（Max Pooling）</strong>：在窗口内找到最大值作为输出</li>
</ul>
</li>
<li><p>在卷积操作之后进行，<strong>本质是采样</strong>，输入的特征图经过池化层后输出的尺寸会减小（《&#x3D;减少了网络的参数数量&#x3D;》），防止过拟合</p>
</li>
</ul>
<h3 id="6-其他多种卷积操作"><a href="#6-其他多种卷积操作" class="headerlink" title="6.其他多种卷积操作"></a>6.其他多种卷积操作</h3><h4 id="（1）反卷积（转置卷积、逆卷积）"><a href="#（1）反卷积（转置卷积、逆卷积）" class="headerlink" title="（1）反卷积（转置卷积、逆卷积）"></a>（1）反卷积（转置卷积、逆卷积）</h4><ul>
<li><p><strong>上采样</strong>：使输入较小的图像变为输出更大的图像</p>
<ul>
<li><p>常用方法：单线性插值、双线性插值（人为设计的，不能很好地适配）</p>
</li>
<li><p>**反卷积（转置卷积、逆卷积）**应运而生：常用于生成式网络</p>
<ul>
<li><p>卷积的逆操作</p>
</li>
<li><table>
<thead>
<tr>
<th align="center">卷积</th>
<th align="center">反卷积</th>
</tr>
</thead>
<tbody><tr>
<td align="center">多对一的映射关系</td>
<td align="center">一对多的映射关系</td>
</tr>
<tr>
<td align="center"></td>
<td align="center"></td>
</tr>
</tbody></table>
</li>
<li></li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="（2）空洞卷积（膨胀卷积）"><a href="#（2）空洞卷积（膨胀卷积）" class="headerlink" title="（2）空洞卷积（膨胀卷积）"></a>（2）空洞卷积（膨胀卷积）</h4><p>除了卷积核尺寸、步长和填充外，引入了一个新参数：<strong>膨胀率</strong></p>
<h4 id="（3）可分离卷积"><a href="#（3）可分离卷积" class="headerlink" title="（3）可分离卷积"></a>（3）可分离卷积</h4><ul>
<li><strong>空间可分离卷积</strong>：（宽高维度）在空间维度上对卷积核进行拆分——》一个卷积运算转化为多个顺序执行的卷积运算</li>
<li><strong>深度可分离卷积</strong>：（通道维度）在通道维度上对标准卷积进行拆分处理</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-comment"># 训练+测试</span><br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> torch.utils.data <span class="hljs-keyword">as</span> data<br><span class="hljs-keyword">import</span> torchvision<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> cv2<br><br>torch.manual_seed(<span class="hljs-number">1</span>) <span class="hljs-comment"># 使用随机化种子使神经网络的初始化每次都相同</span><br><br><span class="hljs-comment"># 超参数</span><br>EPOCH = <span class="hljs-number">1</span> <span class="hljs-comment"># 训练整批数据的次数</span><br>BATCH_SIZE = <span class="hljs-number">50</span><br>LR = <span class="hljs-number">0.001</span> <span class="hljs-comment">#  学习率</span><br>DOWNLOAD_MNIST = <span class="hljs-literal">True</span> <span class="hljs-comment"># 表示还未下载数据集，若数据集下载好了就写False</span><br><br><span class="hljs-comment"># 下载Mnist手写数据集</span><br>train_data = torchvision.datasets.MNIST(<br>    root = <span class="hljs-string">&#x27;./data/data&#x27;</span>, <span class="hljs-comment"># 保存或提取的位置，放在当前文件夹中</span><br>    train = <span class="hljs-literal">True</span>, <span class="hljs-comment"># true表明用于训练的数据，false表示用于测试的数据</span><br>    transform = torchvision.transforms.ToTensor(), <span class="hljs-comment"># 转换PIL.Image or Numpy.ndarray</span><br><br>    download = DOWNLOAD_MNIST, <span class="hljs-comment"># 已经下载了就无需再下载</span><br>)<br><br>test_data = torchvision.datasets.MNIST(<br>root = <span class="hljs-string">&#x27;./data/data&#x27;</span>, <span class="hljs-comment"># 保存或提取的位置，放在当前文件夹中</span><br>    train = <span class="hljs-literal">True</span>, <span class="hljs-comment"># 测试集</span><br>)<br><br><span class="hljs-comment"># 批训练50个samples， 1 channel，28×28（50,1,28,28）</span><br><span class="hljs-comment"># torch中的Dataloader是用来包装数据的工具，能帮我们有效迭代数据，这样就可以进行批训练</span><br>train_loader = data.DataLoader(<br>    dataset = train_data,<br>    batch_size = BATCH_SIZE,<br>    shuffle = <span class="hljs-literal">True</span> <span class="hljs-comment"># 是否打乱数据（一般都打乱）</span><br>)<br><br><span class="hljs-comment"># 进行测试</span><br><span class="hljs-comment"># 为节省时间，测试时只测试前2000个</span><br><br>test_x = torch.unsqueeze(test_data.train_data,dim=<span class="hljs-number">1</span>).<span class="hljs-built_in">type</span>(torch.FloatTensor)[:<span class="hljs-number">2000</span>]/<span class="hljs-number">255</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string"># torch.unsqueeze(a):对数据维度进行扩充，这样shape就从(2000,28,28)-&gt;(2000,1,28,28)</span><br><span class="hljs-string"># 图像的pixel本来是0到255之间，除以255对图像进行归一化使取值范围在（0,1）</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br>test_y = test_data.test_labels[:<span class="hljs-number">2000</span>]<br><br><span class="hljs-comment"># 用class类来建立CNN模型</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string"># CNN流程：卷积(Conv2d)-&gt;激励函数(ReLU)-&gt;池化(Maxpooling)</span><br><span class="hljs-string">#        -&gt;卷积(Conv2d)-&gt;激励函数(ReLU)-&gt;池化(Maxpooling)</span><br><span class="hljs-string">#        -&gt;展平多维的卷积成的特征图-&gt;接入全连接层(Linear)-&gt;输出</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">CNN</span>(nn.Module): <span class="hljs-comment"># 我们建立的CNN集成nn.Module这个模块</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(CNN,<span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-comment"># 建立第一个卷积(Conv2d)-&gt;激励函数(ReLU)-&gt;池化(Maxpooling)</span><br>        <span class="hljs-variable language_">self</span>.conv1 = nn.Sequential(<br>            <span class="hljs-comment"># 第一个卷积con2d</span><br>            nn.Conv2d( <span class="hljs-comment"># 输入图像大小(1,28,28)</span><br>                in_channels = <span class="hljs-number">1</span>, <span class="hljs-comment"># 输入图片的高度，因为minist数据集为灰度图像（只有一个通道）</span><br>                out_channels = <span class="hljs-number">16</span>, <span class="hljs-comment"># n_filters 卷积核的高度</span><br>                kernel_size = <span class="hljs-number">5</span>, <span class="hljs-comment"># filter size 卷积核的大小 也就是长×宽=5×5</span><br>                stride = <span class="hljs-number">1</span>, <span class="hljs-comment"># 步长</span><br>                padding = <span class="hljs-number">2</span>, <span class="hljs-comment"># 填充 使得con2d输出的图像长宽不变 padding = (kernel_size-1)/2</span><br>            ), <span class="hljs-comment"># 输出图像大小(16,28,28)</span><br>            <span class="hljs-comment"># 激活函数</span><br>            nn.ReLU(),<br>            <span class="hljs-comment"># 池化，下采样</span><br>            nn.MaxPool2d(kernel_size = <span class="hljs-number">2</span>), <span class="hljs-comment"># 在2×2空间下采样</span><br>            <span class="hljs-comment"># 输出图像大小(16,14,14)</span><br><br>        )<br><br>        <span class="hljs-comment"># 建立第二个卷积(Conv2d)-&gt;激励函数(ReLU)-&gt;池化(MaxPooling)</span><br>        <span class="hljs-variable language_">self</span>.conv2 = nn.Sequential(<br>            nn.Conv2d( <span class="hljs-comment"># 也可以直接简化，写成nn.Conv2d(16,32,5,1,2)</span><br>            in_channels = <span class="hljs-number">16</span>,<br>            out_channels =<span class="hljs-number">32</span>,<br>            kernel_size = <span class="hljs-number">5</span>,<br>            stride =<span class="hljs-number">1</span>,<br>            padding = <span class="hljs-number">2</span><br>        ),<br>        <span class="hljs-comment"># 输出图像大小(32,14,14)</span><br>            nn.ReLU(),<br>            nn.MaxPool2d(<span class="hljs-number">2</span>)<br>            <span class="hljs-comment"># 输出图像大小(32,7,7)</span><br>        )<br>        <span class="hljs-comment"># 建立全卷积连接层</span><br>        <span class="hljs-variable language_">self</span>.out = nn.Linear(<span class="hljs-number">32</span>*<span class="hljs-number">7</span>*<span class="hljs-number">7</span>,<span class="hljs-number">10</span>) <span class="hljs-comment"># 输出是10个类</span><br><br>        <span class="hljs-comment"># 下面开始定义x的传播路线</span><br>        <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self,x</span>):<br>            x = <span class="hljs-variable language_">self</span>.conv1(x) <span class="hljs-comment"># x先通过conv1</span><br>            x = <span class="hljs-variable language_">self</span>.conv2(x) <span class="hljs-comment"># 再通过conv2</span><br>            <br>            <span class="hljs-comment"># 把每个批次的每一个输入都拉成一个维度，即(batch_size,32*7*7)</span><br>            <span class="hljs-comment"># 因为pytorch里特征的形式是[bs,channel,h,w]，所以x.size(0)就是batchsize</span><br>            <br>            <br>            x = x.view(x.size(<span class="hljs-number">0</span>),-<span class="hljs-number">1</span>) <span class="hljs-comment"># view就是把x弄成batchsize行个tensor</span><br>            output = <span class="hljs-variable language_">self</span>.out(x)<br>            <span class="hljs-keyword">return</span> output<br><br>cnn = CNN()<br><span class="hljs-built_in">print</span>(cnn)<br><br><span class="hljs-comment"># 训练</span><br><span class="hljs-comment"># 把x和y都放入variable中，然后放入cnn中计算output，最后再计算误差</span><br><br><span class="hljs-comment"># 优化器选择Adam</span><br>optimizer = torch.optim.Adam(cnn.parameters(),lr = LR)<br><span class="hljs-comment"># 损失函数</span><br>loss_func = nn.CrossEntropyLoss() <span class="hljs-comment"># 目标标签就是one-hotted</span><br><br><span class="hljs-comment"># 开始训练</span><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(EPOCH):<br>    <span class="hljs-keyword">for</span> step,(b_x,b_y) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(train_loader): <span class="hljs-comment"># 分配batch data</span><br>        output = cnn(b_x) <span class="hljs-comment"># 现将数据放到cnn中计算output</span><br>        loss = loss_func(output,b_y) <span class="hljs-comment"># 输出和真实标签的loss，二者位置不可颠倒</span><br>        optimizer.zero_grad() <span class="hljs-comment"># 清除之前学到的梯度的参数</span><br>        loss.backward() <span class="hljs-comment"># 反向传播，计算梯度</span><br>        optimizer.step() <span class="hljs-comment"># 应用梯度</span><br><br>        <span class="hljs-keyword">if</span> step % <span class="hljs-number">50</span> == <span class="hljs-number">0</span>:<br>            test_output = cnn(test_x)<br>            pred_y = torch.<span class="hljs-built_in">max</span>(test_output,<span class="hljs-number">1</span>)[<span class="hljs-number">1</span>].data.numpy()<br>            accuracy = <span class="hljs-built_in">float</span>((pred_y == test_y.data.numpy()).astype(<span class="hljs-built_in">int</span>).<span class="hljs-built_in">sum</span>())/<span class="hljs-built_in">float</span>(test_y.size(<span class="hljs-number">0</span>))<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Epoch:&#x27;</span>,epoch,<span class="hljs-string">&#x27;| train loss: %.4f&#x27;</span> % loss.data.numpy(),<span class="hljs-string">&#x27;|test.accuracy:%.2f&#x27;</span> % accuracy)<br>torch.save(cnn.state_dict(),<span class="hljs-string">&#x27;cnn.pkl&#x27;</span>) <span class="hljs-comment"># 保存模型</span><br></code></pre></td></tr></table></figure>




                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" class="print-no-link">#学习笔记</a>
      
        <a href="/tags/Markdown/" class="print-no-link">#Markdown</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>神经网络</div>
      <div>http://example.com/2025/04/24/神经网络/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>John Doe</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2025年4月24日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2025/04/24/JavaScript%E5%AD%A6%E4%B9%A0/" title="JavaScript学习">
                        <span class="hidden-mobile">JavaScript学习</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="leancloud-site-pv-container" style="display: none">
        总访问量 
        <span id="leancloud-site-pv"></span>
         次
      </span>
    
    
      <span id="leancloud-site-uv-container" style="display: none">
        总访客数 
        <span id="leancloud-site-uv"></span>
         人
      </span>
    
    

  

</div>

  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script defer src="/js/leancloud.js" ></script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
